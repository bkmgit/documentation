# 3.3.1 Provision your server nodes with SSH access

{% hint style="info" %}
**Recomennded requirements for each server:**\
**16GB RAM / 8CPUs / 320 GB Disk / Ubuntu 20.04 (LTS) x64**
{% endhint %}

{% hint style="warning" %}
In **production**, we recommend that SSH key access to servers is managed using a bastion that includes features for administrators that promote infrastructure security, including key management and auditing.  A good OpenSource Bastion is [Bastillion](https://www.bastillion.io/index.html).
{% endhint %}

1. Using your hosting provider, setup **1, 3** or **5** Ubuntu server nodes **with an additional backup server node in production**. Take note of all generated IP addresses and server hostnames.  \

2. Decide which of your IP addresses will be the **manager server node.** This server will be the manager in the Docker Swarm and the main server you will regularly SSH into to perform commands in this documentation.\

3.  SSH into your server and add all other developers who require server access' [SSH id\_rsa.pub key](https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-2) to all servers' **.ssh/authorized\_keys** file.  Ensure that you take a note of each server's **hostname.** This is a handy command to copy any SSH keys you use already in your team on Git into a server's .ssh/authorized\_keys file.\


    ```
    curl https://github.com/<git-user>.keys >> ~/.ssh/authorized_keys
    ```


4.  For production deployments of 3 or 5 servers, ensure that the **manager server node** can ssh into all the other servers by itself if required in a bash script.\
    \
    SSH into **manager server node** and[ create an ssh key](https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-2).  Press Enter for defaults and no passphrase\


    ```
    ssh-keygen
    ```

    \
    Print the key for copying: \


    ```
    cat ~/.ssh/id_rsa.pub
    ```

    \
    Copy the key and exit the manager node.\
    \
    SSH into each of the 2 or 4 worker nodes to add the key into their **.ssh/authorised\_keys** file \


    ```
    echo "<manager-node-public-key>" >> ~/.ssh/authorized_keys
    ```

    \
    SSH into the **manager server node**, and confirm that you can SSH into all nodes from inside the manager server node.



You are now ready to exit all nodes and run the Ansible command from your local environment to install the required dependencies on the servers.  To set up a backup server in production, refer to the next step.

### Setting up a backup server for production

{% hint style="info" %}
In the next step, your servers will be configured.  In this configuration process, OpenCRVS can optionally set up a scheduled task in Ubuntu's crontab to backup OpenCRVS every night to another external server. &#x20;
{% endhint %}

1. If you are setting up a backup server, **SSH into the backup server** to add the  **manager server node's** key into its .ssh/authorised\_keys file.
2. Create a directory to store OpenCRVS backups, e.g /root/opencrvs
3. **SSH into the manager server node** and wnsure that  the **manager server** can ssh into the backup server.&#x20;

